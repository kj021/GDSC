{
 "cells": [
  {
   "cell_type": "raw",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "21e634194356eff6"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\20161\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 10689718292042749864\n",
      "xla_global_id: -1\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices() )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-19T02:34:12.812056800Z",
     "start_time": "2024-02-19T02:34:10.090275400Z"
    }
   },
   "id": "3b79702222e7db8a"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-02-19T02:41:29.805020200Z",
     "start_time": "2024-02-19T02:41:26.954340300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\20161\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "#import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from time import perf_counter\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report,accuracy_score\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 37345 images belonging to 4 classes.\n",
      "Found 2402 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   validation_split=0.2)\n",
    "\n",
    "train_gen = train_datagen.flow_from_directory('E:/PycharmProjects/peslab/GDSC SC/recycle_img/Training',\n",
    "                                                 target_size = (150, 150),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'categorical',subset='training')\n",
    "val_gen  = train_datagen.flow_from_directory('E:/PycharmProjects/peslab/GDSC SC/recycle_img/Validation',\n",
    "                                                 target_size = (150, 150),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'categorical',subset='validation')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-19T02:41:31.928028600Z",
     "start_time": "2024-02-19T02:41:31.096039900Z"
    }
   },
   "id": "d86a2820ab7cbf04"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\20161\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "WARNING:tensorflow:From C:\\Users\\20161\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "WARNING:tensorflow:From C:\\Users\\20161\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 148, 148, 32)      896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 74, 74, 32)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 72, 72, 32)        9248      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 36, 36, 32)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 41472)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               5308544   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 4)                 516       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5319204 (20.29 MB)\n",
      "Trainable params: 5319204 (20.29 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Initialising the CNN\n",
    "cnn = tf.keras.models.Sequential()\n",
    "\n",
    "# Step 1 - Convolution\n",
    "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=[150, 150, 3]))\n",
    "\n",
    "# Step 2 - Pooling\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n",
    "\n",
    "# Adding convolutional layer\n",
    "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu'))\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n",
    "\n",
    "# Step 3 - Flattening\n",
    "cnn.add(tf.keras.layers.Flatten())\n",
    "\n",
    "# Step 4 - Full Connection\n",
    "cnn.add(tf.keras.layers.Dense(units=128, activation='relu'))\n",
    "\n",
    "# Step 5 - Output Layer\n",
    "cnn.add(tf.keras.layers.Dense(units=4, activation='softmax'))\n",
    "\n",
    "# Compiling the CNN\n",
    "cnn.compile(optimizer = 'adam', \n",
    "            loss = 'categorical_crossentropy', \n",
    "            metrics = ['accuracy'])\n",
    "cnn.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-14T11:18:45.316992600Z",
     "start_time": "2024-02-14T11:18:45.050631400Z"
    }
   },
   "id": "e7031fc088e07eaa"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:From C:\\Users\\20161\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "WARNING:tensorflow:From C:\\Users\\20161\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "1168/1168 [==============================] - 7578s 6s/step - loss: 0.7541 - accuracy: 0.6571 - val_loss: 0.9952 - val_accuracy: 0.5608\n",
      "Epoch 2/10\n",
      "1168/1168 [==============================] - 6663s 6s/step - loss: 0.5335 - accuracy: 0.7651 - val_loss: 1.0653 - val_accuracy: 0.5545\n",
      "Epoch 3/10\n",
      "1168/1168 [==============================] - 6708s 6s/step - loss: 0.3984 - accuracy: 0.8304 - val_loss: 1.1853 - val_accuracy: 0.5966\n",
      "Epoch 4/10\n",
      "1168/1168 [==============================] - 6735s 6s/step - loss: 0.2845 - accuracy: 0.8813 - val_loss: 1.3758 - val_accuracy: 0.6012\n",
      "Epoch 5/10\n",
      "1168/1168 [==============================] - 6700s 6s/step - loss: 0.2027 - accuracy: 0.9189 - val_loss: 1.7324 - val_accuracy: 0.5949\n",
      "Epoch 6/10\n",
      "1168/1168 [==============================] - 6712s 6s/step - loss: 0.1346 - accuracy: 0.9480 - val_loss: 2.3523 - val_accuracy: 0.5783\n",
      "Epoch 7/10\n",
      "1168/1168 [==============================] - 6718s 6s/step - loss: 0.0852 - accuracy: 0.9679 - val_loss: 2.8799 - val_accuracy: 0.5733\n",
      "Epoch 8/10\n",
      "1168/1168 [==============================] - 6721s 6s/step - loss: 0.0579 - accuracy: 0.9791 - val_loss: 2.6825 - val_accuracy: 0.5758\n",
      "Epoch 9/10\n",
      "1168/1168 [==============================] - 7263s 6s/step - loss: 0.0555 - accuracy: 0.9804 - val_loss: 3.8615 - val_accuracy: 0.5566\n",
      "Epoch 10/10\n",
      "1168/1168 [==============================] - 7116s 6s/step - loss: 0.0428 - accuracy: 0.9855 - val_loss: 3.5161 - val_accuracy: 0.5941\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.src.callbacks.History at 0x1bcbe1d94d0>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.fit(x = train_gen, validation_data = val_gen, epochs = 10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-15T06:27:25.483910Z",
     "start_time": "2024-02-14T11:18:46.336784Z"
    }
   },
   "id": "71dfc2464c5c2501"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "cnn.save('E:/PycharmProjects/peslab/GDSC SC/cnn_recycle.h5')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-15T06:51:47.896884700Z",
     "start_time": "2024-02-15T06:51:45.559319400Z"
    }
   },
   "id": "398553c14cad421a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "RESNET 18"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d92d345b92969900"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 150, 150, 3)]        0         []                            \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)           (None, 75, 75, 64)           9472      ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization (Batch  (None, 75, 75, 64)           256       ['conv2d_2[0][0]']            \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation (Activation)     (None, 75, 75, 64)           0         ['batch_normalization[0][0]'] \n",
      "                                                                                                  \n",
      " max_pooling2d_2 (MaxPoolin  (None, 38, 38, 64)           0         ['activation[0][0]']          \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)           (None, 38, 38, 64)           36928     ['max_pooling2d_2[0][0]']     \n",
      "                                                                                                  \n",
      " batch_normalization_1 (Bat  (None, 38, 38, 64)           256       ['conv2d_3[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_1 (Activation)   (None, 38, 38, 64)           0         ['batch_normalization_1[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)           (None, 38, 38, 64)           36928     ['activation_1[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_2 (Bat  (None, 38, 38, 64)           256       ['conv2d_4[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " add (Add)                   (None, 38, 38, 64)           0         ['max_pooling2d_2[0][0]',     \n",
      "                                                                     'batch_normalization_2[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " activation_2 (Activation)   (None, 38, 38, 64)           0         ['add[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)           (None, 38, 38, 64)           36928     ['activation_2[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_3 (Bat  (None, 38, 38, 64)           256       ['conv2d_5[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_3 (Activation)   (None, 38, 38, 64)           0         ['batch_normalization_3[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)           (None, 38, 38, 64)           36928     ['activation_3[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_4 (Bat  (None, 38, 38, 64)           256       ['conv2d_6[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " add_1 (Add)                 (None, 38, 38, 64)           0         ['activation_2[0][0]',        \n",
      "                                                                     'batch_normalization_4[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " activation_4 (Activation)   (None, 38, 38, 64)           0         ['add_1[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)           (None, 19, 19, 128)          73856     ['activation_4[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_6 (Bat  (None, 19, 19, 128)          512       ['conv2d_8[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_5 (Activation)   (None, 19, 19, 128)          0         ['batch_normalization_6[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)           (None, 19, 19, 128)          8320      ['activation_4[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)           (None, 19, 19, 128)          147584    ['activation_5[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_5 (Bat  (None, 19, 19, 128)          512       ['conv2d_7[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_7 (Bat  (None, 19, 19, 128)          512       ['conv2d_9[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " add_2 (Add)                 (None, 19, 19, 128)          0         ['batch_normalization_5[0][0]'\n",
      "                                                                    , 'batch_normalization_7[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " activation_6 (Activation)   (None, 19, 19, 128)          0         ['add_2[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)          (None, 19, 19, 128)          147584    ['activation_6[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_8 (Bat  (None, 19, 19, 128)          512       ['conv2d_10[0][0]']           \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_7 (Activation)   (None, 19, 19, 128)          0         ['batch_normalization_8[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)          (None, 19, 19, 128)          147584    ['activation_7[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_9 (Bat  (None, 19, 19, 128)          512       ['conv2d_11[0][0]']           \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " add_3 (Add)                 (None, 19, 19, 128)          0         ['activation_6[0][0]',        \n",
      "                                                                     'batch_normalization_9[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " activation_8 (Activation)   (None, 19, 19, 128)          0         ['add_3[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)          (None, 10, 10, 256)          295168    ['activation_8[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_11 (Ba  (None, 10, 10, 256)          1024      ['conv2d_13[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_9 (Activation)   (None, 10, 10, 256)          0         ['batch_normalization_11[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)          (None, 10, 10, 256)          33024     ['activation_8[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)          (None, 10, 10, 256)          590080    ['activation_9[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_10 (Ba  (None, 10, 10, 256)          1024      ['conv2d_12[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_12 (Ba  (None, 10, 10, 256)          1024      ['conv2d_14[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " add_4 (Add)                 (None, 10, 10, 256)          0         ['batch_normalization_10[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'batch_normalization_12[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " activation_10 (Activation)  (None, 10, 10, 256)          0         ['add_4[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)          (None, 10, 10, 256)          590080    ['activation_10[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_13 (Ba  (None, 10, 10, 256)          1024      ['conv2d_15[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_11 (Activation)  (None, 10, 10, 256)          0         ['batch_normalization_13[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_16 (Conv2D)          (None, 10, 10, 256)          590080    ['activation_11[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_14 (Ba  (None, 10, 10, 256)          1024      ['conv2d_16[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " add_5 (Add)                 (None, 10, 10, 256)          0         ['activation_10[0][0]',       \n",
      "                                                                     'batch_normalization_14[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " activation_12 (Activation)  (None, 10, 10, 256)          0         ['add_5[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_18 (Conv2D)          (None, 5, 5, 512)            1180160   ['activation_12[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_16 (Ba  (None, 5, 5, 512)            2048      ['conv2d_18[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_13 (Activation)  (None, 5, 5, 512)            0         ['batch_normalization_16[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_17 (Conv2D)          (None, 5, 5, 512)            131584    ['activation_12[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d_19 (Conv2D)          (None, 5, 5, 512)            2359808   ['activation_13[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_15 (Ba  (None, 5, 5, 512)            2048      ['conv2d_17[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_17 (Ba  (None, 5, 5, 512)            2048      ['conv2d_19[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " add_6 (Add)                 (None, 5, 5, 512)            0         ['batch_normalization_15[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'batch_normalization_17[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " activation_14 (Activation)  (None, 5, 5, 512)            0         ['add_6[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_20 (Conv2D)          (None, 5, 5, 512)            2359808   ['activation_14[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_18 (Ba  (None, 5, 5, 512)            2048      ['conv2d_20[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_15 (Activation)  (None, 5, 5, 512)            0         ['batch_normalization_18[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_21 (Conv2D)          (None, 5, 5, 512)            2359808   ['activation_15[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_19 (Ba  (None, 5, 5, 512)            2048      ['conv2d_21[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " add_7 (Add)                 (None, 5, 5, 512)            0         ['activation_14[0][0]',       \n",
      "                                                                     'batch_normalization_19[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " activation_16 (Activation)  (None, 5, 5, 512)            0         ['add_7[0][0]']               \n",
      "                                                                                                  \n",
      " global_average_pooling2d (  (None, 512)                  0         ['activation_16[0][0]']       \n",
      " GlobalAveragePooling2D)                                                                          \n",
      "                                                                                                  \n",
      " dense_2 (Dense)             (None, 4)                    2052      ['global_average_pooling2d[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 11192964 (42.70 MB)\n",
      "Trainable params: 11183364 (42.66 MB)\n",
      "Non-trainable params: 9600 (37.50 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "def residual_block(x, filters, kernel_size=3, stride=1, conv_shortcut=True):\n",
    "    \"\"\"Residual block.\"\"\"\n",
    "    if conv_shortcut:\n",
    "        shortcut = layers.Conv2D(filters, 1, strides=stride, padding='same')(x)\n",
    "        shortcut = layers.BatchNormalization()(shortcut)\n",
    "    else:\n",
    "        shortcut = x\n",
    "\n",
    "    x = layers.Conv2D(filters, kernel_size, strides=stride, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "\n",
    "    x = layers.Conv2D(filters, kernel_size, strides=1, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.add([shortcut, x])\n",
    "    x = layers.Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "def build_resnet18(input_shape=(150, 150, 3), num_classes=4):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = layers.Conv2D(64, 7, strides=2, padding='same')(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.MaxPooling2D(3, strides=2, padding='same')(x)\n",
    "\n",
    "    x = residual_block(x, 64, stride=1, conv_shortcut=False)\n",
    "    x = residual_block(x, 64, stride=1, conv_shortcut=False)\n",
    "\n",
    "    x = residual_block(x, 128, stride=2)\n",
    "    x = residual_block(x, 128, stride=1, conv_shortcut=False)\n",
    "\n",
    "    x = residual_block(x, 256, stride=2)\n",
    "    x = residual_block(x, 256, stride=1, conv_shortcut=False)\n",
    "\n",
    "    x = residual_block(x, 512, stride=2)\n",
    "    x = residual_block(x, 512, stride=1, conv_shortcut=False)\n",
    "\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = models.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "# Build ResNet-18 model\n",
    "resnet18 = build_resnet18(input_shape=[150, 150, 3], num_classes=4)\n",
    "\n",
    "# Compile the model\n",
    "resnet18.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Model summary\n",
    "resnet18.summary()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-15T06:57:51.203437200Z",
     "start_time": "2024-02-15T06:57:50.721085500Z"
    }
   },
   "id": "5c72633c6039882"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1168/1168 [==============================] - 7022s 6s/step - loss: 0.7515 - accuracy: 0.6538 - val_loss: 1.3177 - val_accuracy: 0.5462\n",
      "Epoch 2/10\n",
      "1168/1168 [==============================] - 6873s 6s/step - loss: 0.4853 - accuracy: 0.7893 - val_loss: 1.1761 - val_accuracy: 0.6249\n",
      "Epoch 3/10\n",
      "1168/1168 [==============================] - 6891s 6s/step - loss: 0.3658 - accuracy: 0.8486 - val_loss: 1.0656 - val_accuracy: 0.6628\n",
      "Epoch 4/10\n",
      "1168/1168 [==============================] - 6894s 6s/step - loss: 0.3076 - accuracy: 0.8752 - val_loss: 1.0876 - val_accuracy: 0.6478\n",
      "Epoch 5/10\n",
      "1168/1168 [==============================] - 6899s 6s/step - loss: 0.2450 - accuracy: 0.9017 - val_loss: 1.0956 - val_accuracy: 0.6366\n",
      "Epoch 6/10\n",
      "1168/1168 [==============================] - 6916s 6s/step - loss: 0.2076 - accuracy: 0.9189 - val_loss: 0.9215 - val_accuracy: 0.6790\n",
      "Epoch 7/10\n",
      "1168/1168 [==============================] - 6930s 6s/step - loss: 0.1659 - accuracy: 0.9364 - val_loss: 1.1404 - val_accuracy: 0.6686\n",
      "Epoch 8/10\n",
      "1168/1168 [==============================] - 6910s 6s/step - loss: 0.1346 - accuracy: 0.9482 - val_loss: 0.9975 - val_accuracy: 0.6998\n",
      "Epoch 9/10\n",
      "1168/1168 [==============================] - 6884s 6s/step - loss: 0.1078 - accuracy: 0.9580 - val_loss: 1.2985 - val_accuracy: 0.6699\n",
      "Epoch 10/10\n",
      "1168/1168 [==============================] - 7367s 6s/step - loss: 0.0902 - accuracy: 0.9656 - val_loss: 1.6464 - val_accuracy: 0.6757\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.src.callbacks.History at 0x1bcc963d750>"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet18.fit(x = train_gen, validation_data = val_gen, epochs = 10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-16T02:18:16.538595600Z",
     "start_time": "2024-02-15T06:58:27.031973800Z"
    }
   },
   "id": "e9462df4cfe32fda"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "resnet18.save('E:/PycharmProjects/peslab/GDSC SC/resnet18_recycle.h5')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-16T04:27:46.908461800Z",
     "start_time": "2024-02-16T04:27:41.844606Z"
    }
   },
   "id": "42add18e4f71493"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Efficientnet"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d5005e5c4b86dfeb"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " keras_layer_2 (KerasLayer)  (None, 2048)              23564800  \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 4)                 8196      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 23572996 (89.92 MB)\n",
      "Trainable params: 8196 (32.02 KB)\n",
      "Non-trainable params: 23564800 (89.89 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "# Example: Loading a ResNet model from TensorFlow Hub\n",
    "model_url = 'https://tfhub.dev/google/imagenet/resnet_v2_50/feature_vector/4'\n",
    "hub_layer = hub.KerasLayer(model_url, input_shape=(150, 150, 3))\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    hub_layer,\n",
    "    tf.keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-16T05:47:43.826222500Z",
     "start_time": "2024-02-16T05:47:33.849986900Z"
    }
   },
   "id": "f45c12b4c44da8ee"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1168/1168 [==============================] - 7622s 7s/step - loss: 0.5344 - accuracy: 0.7673 - val_loss: 0.7273 - val_accuracy: 0.7027\n",
      "Epoch 2/10\n",
      "1168/1168 [==============================] - 6926s 6s/step - loss: 0.4274 - accuracy: 0.8132 - val_loss: 0.6647 - val_accuracy: 0.7277\n",
      "Epoch 3/10\n",
      "1168/1168 [==============================] - 6866s 6s/step - loss: 0.3985 - accuracy: 0.8244 - val_loss: 0.6987 - val_accuracy: 0.7107\n",
      "Epoch 4/10\n",
      "1168/1168 [==============================] - 6887s 6s/step - loss: 0.3946 - accuracy: 0.8289 - val_loss: 0.5772 - val_accuracy: 0.7868\n",
      "Epoch 5/10\n",
      "1168/1168 [==============================] - 6862s 6s/step - loss: 0.3839 - accuracy: 0.8313 - val_loss: 0.9111 - val_accuracy: 0.6919\n",
      "Epoch 6/10\n",
      "1168/1168 [==============================] - 6866s 6s/step - loss: 0.3914 - accuracy: 0.8306 - val_loss: 0.7202 - val_accuracy: 0.7448\n",
      "Epoch 7/10\n",
      "1168/1168 [==============================] - 6879s 6s/step - loss: 0.3832 - accuracy: 0.8341 - val_loss: 0.7931 - val_accuracy: 0.7040\n",
      "Epoch 8/10\n",
      "1168/1168 [==============================] - 6857s 6s/step - loss: 0.3818 - accuracy: 0.8340 - val_loss: 0.8793 - val_accuracy: 0.7240\n",
      "Epoch 9/10\n",
      "1168/1168 [==============================] - 6858s 6s/step - loss: 0.3748 - accuracy: 0.8374 - val_loss: 0.7215 - val_accuracy: 0.7444\n",
      "Epoch 10/10\n",
      "1168/1168 [==============================] - 6870s 6s/step - loss: 0.3746 - accuracy: 0.8377 - val_loss: 0.8001 - val_accuracy: 0.7427\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.src.callbacks.History at 0x1bcf5b70c90>"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x = train_gen, validation_data = val_gen, epochs = 10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-17T01:06:11.969576900Z",
     "start_time": "2024-02-16T05:47:53.950595700Z"
    }
   },
   "id": "4f10c18915919b34"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "model.save('E:/PycharmProjects/peslab/GDSC SC/efficientnet_recycle.h5')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-17T01:06:15.221894300Z",
     "start_time": "2024-02-17T01:06:11.969576900Z"
    }
   },
   "id": "83cdf95f7021033d"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " keras_layer_4 (KerasLayer)  (None, 2048)              23564800  \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 512)               1049088   \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 4)                 2052      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 24615940 (93.90 MB)\n",
      "Trainable params: 1051140 (4.01 MB)\n",
      "Non-trainable params: 23564800 (89.89 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "def build_model(num_classes):\n",
    "    # Load a pre-trained ResNet model as the base\n",
    "    base_model_url = \"https://tfhub.dev/google/imagenet/resnet_v2_50/feature_vector/4\"\n",
    "    base_model = hub.KerasLayer(base_model_url, input_shape=(224, 224, 3), trainable=False)\n",
    "\n",
    "    # Build the model\n",
    "    model = models.Sequential([\n",
    "        base_model,\n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Set the number of classes\n",
    "num_classes = 4\n",
    "\n",
    "# Build and compile the model\n",
    "model = build_model(num_classes)\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-18T05:43:09.458858700Z",
     "start_time": "2024-02-18T05:43:07.871748Z"
    }
   },
   "id": "8f529bca1051b673"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "1168/1168 [==============================] - 6900s 6s/step - loss: 0.5038 - accuracy: 0.7761 - val_loss: 0.5650 - val_accuracy: 0.7448\n",
      "Epoch 2/7\n",
      "1168/1168 [==============================] - 6930s 6s/step - loss: 0.3835 - accuracy: 0.8266 - val_loss: 0.6720 - val_accuracy: 0.6982\n",
      "Epoch 3/7\n",
      "1168/1168 [==============================] - 6897s 6s/step - loss: 0.3372 - accuracy: 0.8496 - val_loss: 0.5798 - val_accuracy: 0.7748\n",
      "Epoch 4/7\n",
      "1168/1168 [==============================] - 6906s 6s/step - loss: 0.3012 - accuracy: 0.8690 - val_loss: 0.6143 - val_accuracy: 0.8172\n",
      "Epoch 5/7\n",
      "1168/1168 [==============================] - 6886s 6s/step - loss: 0.2711 - accuracy: 0.8845 - val_loss: 0.7186 - val_accuracy: 0.7619\n",
      "Epoch 6/7\n",
      "1168/1168 [==============================] - 6834s 6s/step - loss: 0.2483 - accuracy: 0.8955 - val_loss: 0.8158 - val_accuracy: 0.7319\n",
      "Epoch 7/7\n",
      "1168/1168 [==============================] - 6808s 6s/step - loss: 0.2215 - accuracy: 0.9073 - val_loss: 0.6972 - val_accuracy: 0.8039\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.src.callbacks.History at 0x1bd2c1a9190>"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x = train_gen, validation_data = val_gen, epochs = 7)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-18T19:05:54.530794200Z",
     "start_time": "2024-02-18T05:43:09.459855400Z"
    }
   },
   "id": "5cc690c84b8dd6a1"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "model.save('E:/PycharmProjects/peslab/GDSC SC/resnet18_epoch7.h5')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-18T19:05:58.185119500Z",
     "start_time": "2024-02-18T19:05:54.531931200Z"
    }
   },
   "id": "ff82660372e7f6b9"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "\"\"\"def create_gen():\n",
    "    # 생성기 및 데이터 증강으로 이미지 로드\n",
    "    train_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "        preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input,\n",
    "        validation_split=0.1\n",
    "    )\n",
    "\n",
    "    test_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "        preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input\n",
    "    )\n",
    "\n",
    "    train_images = train_generator.flow_from_dataframe(\n",
    "        dataframe=train_df,\n",
    "        x_col='Filepath', # 파일위치 열이름\n",
    "        y_col='Label', # 클래스 열이름\n",
    "        target_size=(224, 224), # 이미지 사이즈\n",
    "        color_mode='rgb', # 이미지 채널수\n",
    "        class_mode='categorical', # Y값(Label값)\n",
    "        batch_size=32,\n",
    "        shuffle=True, # 데이터를 섞을지 여부\n",
    "        seed=0,\n",
    "        subset='training', # train 인지 val인지 설정\n",
    "        rotation_range=30, # 회전제한 각도 30도\n",
    "        zoom_range=0.15, # 확대 축소 15%\n",
    "        width_shift_range=0.2, # 좌우이동 20%\n",
    "        height_shift_range=0.2, # 상하이동 20%\n",
    "        shear_range=0.15, # 반시계방햐의 각도\n",
    "        horizontal_flip=True, # 좌우 반전 True\n",
    "        fill_mode=\"nearest\"\n",
    "        # 이미지 변경시 보완 방법 (constant, nearest, reflect, wrap) 4개 존재\n",
    "    )\n",
    "\n",
    "    val_images = train_generator.flow_from_dataframe(\n",
    "        dataframe=train_df,\n",
    "        x_col='Filepath',\n",
    "        y_col='Label',\n",
    "        target_size=(224, 224),\n",
    "        color_mode='rgb',\n",
    "        class_mode='categorical',\n",
    "        batch_size=32,\n",
    "        shuffle=True,\n",
    "        seed=0,\n",
    "        subset='validation',\n",
    "        rotation_range=30,\n",
    "        zoom_range=0.15,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.15,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode=\"nearest\"\n",
    "    )\n",
    "\n",
    "    test_images = test_generator.flow_from_dataframe(\n",
    "        dataframe=test_df,\n",
    "        x_col='Filepath',\n",
    "        y_col='Label',\n",
    "        target_size=(224, 224),\n",
    "        color_mode='rgb',\n",
    "        class_mode='categorical',\n",
    "        batch_size=32,\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    return train_generator,test_generator,train_images,val_images,test_images\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-15T06:27:25.490556800Z",
     "start_time": "2024-02-15T06:27:25.486900200Z"
    }
   },
   "id": "b192a7a8532f5eb9"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "def create_gen():\n",
    "    # Initialize ImageDataGenerator for training with data augmentation and rescaling\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input, # Preprocessing function\n",
    "        rescale=1./255, # Rescaling factor\n",
    "        validation_split=0.2, # Splitting data for validation\n",
    "        rotation_range=30, # Random rotation\n",
    "        zoom_range=0.15, # Random zoom\n",
    "        width_shift_range=0.2, # Random width shift\n",
    "        height_shift_range=0.2, # Random height shift\n",
    "        shear_range=0.15, # Shearing\n",
    "        horizontal_flip=True, # Horizontal flip\n",
    "        fill_mode=\"nearest\" # Fill mode for newly created pixels\n",
    "    )\n",
    "\n",
    "    # Setup training generator\n",
    "    train_gen = train_datagen.flow_from_directory(\n",
    "        'E:/PycharmProjects/peslab/GDSC SC/recycle_img/Training', # Training directory path\n",
    "        target_size=(224, 224), # Target image size\n",
    "        batch_size=32, # Batch size\n",
    "        class_mode='categorical', # Class mode\n",
    "        shuffle=True, # Shuffle data\n",
    "        subset='training' # Subset for training\n",
    "    )\n",
    "\n",
    "    # Setup validation generator\n",
    "    val_gen = train_datagen.flow_from_directory(\n",
    "        'E:/PycharmProjects/peslab/GDSC SC/recycle_img/Validation', # Validation directory path\n",
    "        target_size=(224, 224), # Target image size\n",
    "        batch_size=32, # Batch size\n",
    "        class_mode='categorical', # Class mode\n",
    "        shuffle=True, # Shuffle data\n",
    "        subset='validation' # Subset for validation\n",
    "    )\n",
    "\n",
    "    return train_gen, val_gen\n",
    "\n",
    "#train_gen, val_gen = create_gen()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-19T02:41:39.406544300Z",
     "start_time": "2024-02-19T02:41:39.352373700Z"
    }
   },
   "id": "62dbeeb98ff2a94d"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 37345 images belonging to 4 classes.\n",
      "Found 2402 images belonging to 4 classes.\n",
      "\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\20161\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "WARNING:tensorflow:From C:\\Users\\20161\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\normalization\\batch_normalization.py:979: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "WARNING:tensorflow:From C:\\Users\\20161\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "WARNING:tensorflow:From C:\\Users\\20161\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "WARNING:tensorflow:From C:\\Users\\20161\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "1168/1168 [==============================] - 7684s 7s/step - loss: 1.0346 - accuracy: 0.5144 - val_loss: 1.4166 - val_accuracy: 0.4967\n",
      "DenseNet121          trained in 7690.95 sec\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
      "9406464/9406464 [==============================] - 1s 0us/step\n",
      "1168/1168 [==============================] - 7670s 7s/step - loss: 1.0391 - accuracy: 0.5110 - val_loss: 1.5059 - val_accuracy: 0.4967\n",
      "MobileNetV2          trained in 7673.25 sec\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet201_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "74836368/74836368 [==============================] - 7s 0us/step\n",
      "1168/1168 [==============================] - 10346s 9s/step - loss: 1.0254 - accuracy: 0.5152 - val_loss: 1.4879 - val_accuracy: 0.4967\n",
      "DenseNet201          trained in 10351.04 sec\n",
      "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n",
      "16705208/16705208 [==============================] - 2s 0us/step\n",
      "1168/1168 [==============================] - 7063s 6s/step - loss: 1.0443 - accuracy: 0.5145 - val_loss: 1.4133 - val_accuracy: 0.4967\n",
      "EfficientNetB0       trained in 7067.6 sec\n",
      "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb1_notop.h5\n",
      "27018416/27018416 [==============================] - 3s 0us/step\n",
      "1168/1168 [==============================] - 7067s 6s/step - loss: 1.0438 - accuracy: 0.5143 - val_loss: 1.5829 - val_accuracy: 0.4967\n",
      "EfficientNetB1       trained in 7070.47 sec\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "87910968/87910968 [==============================] - 8s 0us/step\n",
      "1168/1168 [==============================] - 7132s 6s/step - loss: 0.8924 - accuracy: 0.5470 - val_loss: 0.8915 - val_accuracy: 0.5679\n",
      "InceptionV3          trained in 7136.12 sec\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v3/weights_mobilenet_v3_large_224_1.0_float_no_top_v2.h5\n",
      "12683000/12683000 [==============================] - 1s 0us/step\n",
      "1168/1168 [==============================] - 7004s 6s/step - loss: 1.0424 - accuracy: 0.5161 - val_loss: 1.5555 - val_accuracy: 0.4967\n",
      "MobileNetV3Large     trained in 7008.37 sec\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet152v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "234545216/234545216 [==============================] - 20s 0us/step\n",
      "1168/1168 [==============================] - 7149s 6s/step - loss: 0.8964 - accuracy: 0.5519 - val_loss: 0.9613 - val_accuracy: 0.6003\n",
      "ResNet152V2          trained in 7154.83 sec\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "94765736/94765736 [==============================] - 8s 0us/step\n",
      "1168/1168 [==============================] - 6950s 6s/step - loss: 1.0451 - accuracy: 0.5105 - val_loss: 1.6529 - val_accuracy: 0.4967\n",
      "ResNet50             trained in 6954.13 sec\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "94668760/94668760 [==============================] - 9s 0us/step\n",
      "1168/1168 [==============================] - 6835s 6s/step - loss: 0.9067 - accuracy: 0.5429 - val_loss: 1.0776 - val_accuracy: 0.5221\n",
      "ResNet50V2           trained in 6838.46 sec\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "80134624/80134624 [==============================] - 8s 0us/step\n",
      "1168/1168 [==============================] - 10047s 9s/step - loss: 1.0423 - accuracy: 0.5153 - val_loss: 1.5498 - val_accuracy: 0.4967\n",
      "VGG19                trained in 10051.69 sec\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58889256/58889256 [==============================] - 5s 0us/step\n",
      " 601/1168 [==============>...............] - ETA: 1:35:56 - loss: 1.0447 - accuracy: 0.5112"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    \"DenseNet121\": {\"model\":tf.keras.applications.DenseNet121, \"perf\":0},\n",
    "    \"MobileNetV2\": {\"model\":tf.keras.applications.MobileNetV2, \"perf\":0},\n",
    "    \"DenseNet201\": {\"model\":tf.keras.applications.DenseNet201, \"perf\":0},\n",
    "    \"EfficientNetB0\": {\"model\":tf.keras.applications.EfficientNetB0, \"perf\":0},\n",
    "    \"EfficientNetB1\": {\"model\":tf.keras.applications.EfficientNetB1, \"perf\":0},\n",
    "    \"InceptionV3\": {\"model\":tf.keras.applications.InceptionV3, \"perf\":0},\n",
    "    \"MobileNetV2\": {\"model\":tf.keras.applications.MobileNetV2, \"perf\":0},\n",
    "    \"MobileNetV3Large\": {\"model\":tf.keras.applications.MobileNetV3Large, \"perf\":0},\n",
    "    \"ResNet152V2\": {\"model\":tf.keras.applications.ResNet152V2, \"perf\":0},\n",
    "    \"ResNet50\": {\"model\":tf.keras.applications.ResNet50, \"perf\":0},\n",
    "    \"ResNet50V2\": {\"model\":tf.keras.applications.ResNet50V2, \"perf\":0},\n",
    "    \"VGG19\": {\"model\":tf.keras.applications.VGG19, \"perf\":0},\n",
    "    \"VGG16\": {\"model\":tf.keras.applications.VGG16, \"perf\":0},\n",
    "    \"Xception\": {\"model\":tf.keras.applications.Xception, \"perf\":0}\n",
    "}\n",
    "# Create the generators\n",
    "#train_generator,test_generator,train_images,val_images,test_images=create_gen()\n",
    "train_gen, val_gen = create_gen()\n",
    "print('\\n')\n",
    "\n",
    "def get_model(model):\n",
    "# Load the pretained model\n",
    "    kwargs =    {'input_shape':(224, 224, 3),\n",
    "                'include_top':False,\n",
    "                'weights':'imagenet',\n",
    "                'pooling':'avg'}\n",
    "    \n",
    "    pretrained_model = model(**kwargs)\n",
    "    pretrained_model.trainable = False # 레이어를 동결 시켜서 훈련중 손실을 최소화 한다.\n",
    "    \n",
    "    inputs = pretrained_model.input\n",
    "\n",
    "    x = tf.keras.layers.Dense(128, activation='relu')(pretrained_model.output)\n",
    "    x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "\n",
    "    outputs = tf.keras.layers.Dense(4, activation='softmax')(x)\n",
    "    # 라벨 개수가 4개이기 때문에 Dencs도 4로 설정\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Train모델 학습\n",
    "for name, model in models.items():\n",
    "    \n",
    "    # 전이 학습 모델 가져오기\n",
    "    m = get_model(model['model'])\n",
    "    models[name]['model'] = m\n",
    "    \n",
    "    start = perf_counter()\n",
    "    \n",
    "    # 모델 학습\n",
    "    history = m.fit(train_gen,validation_data=val_gen,epochs=1,verbose=1)\n",
    "    \n",
    "    # 학습시간과 val_accuracy 저장\n",
    "    duration = perf_counter() - start\n",
    "    duration = round(duration,2)\n",
    "    models[name]['perf'] = duration\n",
    "    print(f\"{name:20} trained in {duration} sec\")\n",
    "    \n",
    "    val_acc = history.history['val_accuracy']\n",
    "    models[name]['val_acc'] = [round(v,4) for v in val_acc]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-20T04:01:46.142255600Z",
     "start_time": "2024-02-19T02:41:40.118771Z"
    }
   },
   "id": "ff08ef7ad9d9c016"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# test데이터로 모델 성능 예측\n",
    "for name, model in models.items():\n",
    "    \n",
    "    # Predict the label of the test_images\n",
    "    pred = models[name]['model'].predict(test_images)\n",
    "    pred = np.argmax(pred,axis=1)\n",
    "\n",
    "    # Map the label\n",
    "    labels = (train_images.class_indices)\n",
    "    labels = dict((v,k) for k,v in labels.items())\n",
    "    pred = [labels[k] for k in pred]\n",
    "\n",
    "    y_test = list(test_df.Label)\n",
    "    acc = accuracy_score(y_test,pred)\n",
    "    models[name]['acc'] = round(acc,4)\n",
    "    print(f'**{name} has a {acc * 100:.2f}% accuracy on the test set**')\n",
    "   \n",
    "# Create a DataFrame with the results\n",
    "models_result = []\n",
    "\n",
    "for name, v in models.items():\n",
    "    models_result.append([ name, models[name]['val_acc'][-1], \n",
    "                          models[name]['acc'],\n",
    "                          models[name]['perf']])\n",
    "    \n",
    "df_results = pd.DataFrame(models_result, \n",
    "                          columns = ['model','val_accuracy','accuracy','Training time (sec)'])\n",
    "df_results.sort_values(by='accuracy', ascending=False, inplace=True)\n",
    "df_results.reset_index(inplace=True,drop=True)\n",
    "df_results"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-15T06:27:25.537821500Z",
     "start_time": "2024-02-15T06:27:25.537821500Z"
    }
   },
   "id": "49ba5cfcd8af3a01"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,5))\n",
    "sns.barplot(x = 'model', y = 'accuracy', data = df_results)\n",
    "plt.title('Accuracy on the test set (after 1 epoch))', fontsize = 15)\n",
    "plt.ylim(0,1)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-15T06:27:25.538817600Z",
     "start_time": "2024-02-15T06:27:25.538817600Z"
    }
   },
   "id": "8cde84bad430809d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,5))\n",
    "sns.barplot(x = 'model', y = 'Training time (sec)', data = df_results)\n",
    "plt.title('Training time for each model in sec', fontsize = 15)\n",
    "# plt.ylim(0,20)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-15T06:27:25.539814500Z"
    }
   },
   "id": "bf1f348864c2035f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_df,test_df = train_test_split(df, test_size=0.1, random_state=0)\n",
    "train_generator,test_generator,train_images,val_images,test_images=create_gen()\n",
    "\n",
    "model = get_model(tf.keras.applications.DenseNet201)\n",
    "history = model.fit(train_images,validation_data=val_images,epochs=7)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-15T06:27:25.540811400Z"
    }
   },
   "id": "1dfe5f1f9ff32acf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pd.DataFrame(history.history)[['accuracy','val_accuracy']].plot()\n",
    "plt.title(\"Accuracy\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-15T06:27:25.541807700Z"
    }
   },
   "id": "caf1edfc936137a0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pd.DataFrame(history.history)[['loss','val_loss']].plot()\n",
    "plt.title(\"Loss\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-15T06:27:25.543801Z"
    }
   },
   "id": "8b33185081a3594a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Predict the label of the test_images\n",
    "pred = model.predict(test_images)\n",
    "pred = np.argmax(pred,axis=1)\n",
    "\n",
    "# Map the label\n",
    "labels = (train_images.class_indices)\n",
    "labels = dict((v,k) for k,v in labels.items())\n",
    "pred = [labels[k] for k in pred]\n",
    "    \n",
    "y_test = list(test_df.Label)\n",
    "acc = accuracy_score(y_test,pred)\n",
    "print(f'Accuracy on the test set: {acc * 100:.2f}%')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-15T06:27:25.544797900Z"
    }
   },
   "id": "de91ed8cdcbadecf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_df,test_df = train_test_split(df, test_size=0.1, random_state=0)\n",
    "train_generator,test_generator,train_images,val_images,test_images=create_gen()\n",
    "\n",
    "model = get_model(tf.keras.applications.ResNet152V2)\n",
    "history = model.fit(train_images,validation_data=val_images,epochs=5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-15T06:27:25.545794500Z"
    }
   },
   "id": "b23fd46d8a089e6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pd.DataFrame(history.history)[['accuracy','val_accuracy']].plot()\n",
    "plt.title(\"Accuracy\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-15T06:27:25.546791Z"
    }
   },
   "id": "ab8a6388b96f4476"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pd.DataFrame(history.history)[['loss','val_loss']].plot()\n",
    "plt.title(\"Loss\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-15T06:27:25.547788100Z"
    }
   },
   "id": "7a09ca346a7ffd80"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Predict the label of the test_images\n",
    "pred = model.predict(test_images)\n",
    "pred = np.argmax(pred,axis=1)\n",
    "\n",
    "# Map the label\n",
    "labels = (train_images.class_indices)\n",
    "labels = dict((v,k) for k,v in labels.items())\n",
    "pred = [labels[k] for k in pred]\n",
    "\n",
    "def printmd(string):\n",
    "    # Print with Markdowns    \n",
    "    display(Markdown(string))\n",
    "    \n",
    "y_test = list(test_df.Label)\n",
    "acc = accuracy_score(y_test,pred)\n",
    "printmd(f'# Accuracy on the test set: {acc * 100:.2f}%')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-15T06:27:25.547788100Z"
    }
   },
   "id": "2a480c8b49ac76dc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class_report = classification_report(y_test, pred, zero_division=1)\n",
    "print(class_report)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-15T06:27:25.548784400Z"
    }
   },
   "id": "e98348841fb9f2bc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "cf_matrix = confusion_matrix(y_test, pred, normalize='true')\n",
    "plt.figure(figsize = (10,7))\n",
    "sns.heatmap(cf_matrix, annot=False, xticklabels = sorted(set(y_test)), yticklabels = sorted(set(y_test)),cbar=False)\n",
    "plt.title('Normalized Confusion Matrix', fontsize = 23)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-15T06:27:25.549781200Z"
    }
   },
   "id": "16a712d7e7d4f90d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.applications.inception_resnet_v2 import InceptionResNetV2, preprocess_input\n",
    "def printmd(string):\n",
    "    # Print with Markdowns    \n",
    "    display(Markdown(string))\n",
    "class_dictionary = {'airplane': 0,\n",
    "                    'car': 1,\n",
    "                    'cat': 2,\n",
    "                    'dog': 3,\n",
    "                    'flower': 4,\n",
    "                    'fruit': 5,\n",
    "                    'motorbike': 6,\n",
    "                    'person': 7}\n",
    "IMAGE_SIZE    = (224, 224)\n",
    "number_1 = int(input(\"번호를 입력하세요 : \")) # 10, 50, 100\n",
    "test_image = image.load_img(test_df.iloc[number_1, 0]\n",
    "                            ,target_size =IMAGE_SIZE )\n",
    "test_image = image.img_to_array(test_image)\n",
    "plt.imshow(test_image/255.);\n",
    "\n",
    "test_image = test_image.reshape((1, test_image.shape[0], test_image.shape[1], test_image.shape[2]))\n",
    "test_image = preprocess_input(test_image)\n",
    "prediction = model.predict(test_image)\n",
    "\n",
    "df = pd.DataFrame({'pred':prediction[0]})\n",
    "df = df.sort_values(by='pred', ascending=False, na_position='first')\n",
    "printmd(f\"## 예측률 : {(df.iloc[0]['pred'])* 100:.2f}%\")\n",
    "\n",
    "for x in class_dictionary:\n",
    "  if class_dictionary[x] == (df[df == df.iloc[0]].index[0]):\n",
    "    printmd(f\"### Class prediction = {x}\")\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-15T06:27:25.550777600Z"
    }
   },
   "id": "f19db5a478df9e17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Display picture of the dataset with their labels\n",
    "fig, axes = plt.subplots(nrows=4, ncols=6, figsize=(20, 12),\n",
    "                        subplot_kw={'xticks': [], 'yticks': []})\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(plt.imread(test_df.Filepath.iloc[i]))\n",
    "    ax.set_title(f\"True: {test_df.Label.iloc[i].split('_')[0]}\\nPredicted: {pred[i].split('_')[0]}\", fontsize = 15)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-15T06:27:25.550777600Z"
    }
   },
   "id": "e6aae5939e6e7fa8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "이미지 한장을 가지고 성능 테스트"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f49736f09bcf7401"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 368ms/step\n",
      "Predicted class: [0]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "\n",
    "# 1. 모델 불러오기\n",
    "model = load_model('E:/PycharmProjects/peslab/GDSC SC/resnet18_epoch7.h5', custom_objects={'KerasLayer': hub.KerasLayer})\n",
    "\n",
    "# 2. 이미지 전처리\n",
    "def preprocess_image(img_path, target_size=(224, 224)):\n",
    "    img = image.load_img(img_path, target_size=target_size) # 이미지 로드 및 크기 조정\n",
    "    img_array = image.img_to_array(img) # 이미지를 numpy 배열로 변환\n",
    "    img_array_expanded_dims = np.expand_dims(img_array, axis=0) # 차원 확장\n",
    "    return tf.keras.applications.mobilenet.preprocess_input(img_array_expanded_dims) # 모델에 맞게 전처리\n",
    "\n",
    "# 3. 이미지를 모델에 입력하고 예측 수행\n",
    "img_path = 'C:/Users/20161/Downloads/건전지.jpg' \n",
    "preprocessed_image = preprocess_image(img_path)\n",
    "predictions = model.predict(preprocessed_image)\n",
    "\n",
    "# 4. 예측 결과 출력 (여기서는 가장 높은 확률을 가진 클래스를 출력합니다)\n",
    "predicted_class = np.argmax(predictions, axis=1)\n",
    "print(\"Predicted class:\", predicted_class)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-20T04:12:44.859623900Z",
     "start_time": "2024-02-20T04:12:43.335351200Z"
    }
   },
   "id": "fd64492d95df674a"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 370ms/step\n",
      "Predicted class: [1]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "\n",
    "# 1. 모델 불러오기\n",
    "model = load_model('E:/PycharmProjects/peslab/GDSC SC/resnet18_epoch7.h5', custom_objects={'KerasLayer': hub.KerasLayer})\n",
    "\n",
    "# 2. 이미지 전처리\n",
    "def preprocess_image(img_path, target_size=(224, 224)):\n",
    "    img = image.load_img(img_path, target_size=target_size) # 이미지 로드 및 크기 조정\n",
    "    img_array = image.img_to_array(img) # 이미지를 numpy 배열로 변환\n",
    "    img_array_expanded_dims = np.expand_dims(img_array, axis=0) # 차원 확장\n",
    "    return tf.keras.applications.mobilenet.preprocess_input(img_array_expanded_dims) # 모델에 맞게 전처리\n",
    "\n",
    "# 3. 이미지를 모델에 입력하고 예측 수행\n",
    "img_path = 'C:/Users/20161/Downloads/스티로폼1.jpg' \n",
    "preprocessed_image = preprocess_image(img_path)\n",
    "predictions = model.predict(preprocessed_image)\n",
    "\n",
    "# 4. 예측 결과 출력 (여기서는 가장 높은 확률을 가진 클래스를 출력합니다)\n",
    "predicted_class = np.argmax(predictions, axis=1)\n",
    "print(\"Predicted class:\", predicted_class)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-20T04:28:50.195034200Z",
     "start_time": "2024-02-20T04:28:48.656286500Z"
    }
   },
   "id": "16d79509551c400c"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000159E4451F80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000159E4451F80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 367ms/step\n",
      "Predicted class: [1]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "\n",
    "# 1. 모델 불러오기\n",
    "model = load_model('E:/PycharmProjects/peslab/GDSC SC/resnet18_epoch7.h5', custom_objects={'KerasLayer': hub.KerasLayer})\n",
    "\n",
    "# 2. 이미지 전처리\n",
    "def preprocess_image(img_path, target_size=(224, 224)):\n",
    "    img = image.load_img(img_path, target_size=target_size) # 이미지 로드 및 크기 조정\n",
    "    img_array = image.img_to_array(img) # 이미지를 numpy 배열로 변환\n",
    "    img_array_expanded_dims = np.expand_dims(img_array, axis=0) # 차원 확장\n",
    "    return tf.keras.applications.mobilenet.preprocess_input(img_array_expanded_dims) # 모델에 맞게 전처리\n",
    "\n",
    "# 3. 이미지를 모델에 입력하고 예측 수행\n",
    "img_path = 'C:/Users/20161/Downloads/유리병1.jpg' \n",
    "preprocessed_image = preprocess_image(img_path)\n",
    "predictions = model.predict(preprocessed_image)\n",
    "\n",
    "# 4. 예측 결과 출력 (여기서는 가장 높은 확률을 가진 클래스를 출력합니다)\n",
    "predicted_class = np.argmax(predictions, axis=1)\n",
    "print(\"Predicted class:\", predicted_class)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-20T04:31:21.440554Z",
     "start_time": "2024-02-20T04:31:19.796202Z"
    }
   },
   "id": "d7313989aac3d63e"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000159F087D760> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000159F087D760> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 369ms/step\n",
      "Predicted class: [3]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras.models import load_model%\n",
    "\n",
    "\n",
    "# 1. 모델 불러오기\n",
    "model = load_model('E:/PycharmProjects/peslab/GDSC SC/resnet18_epoch7.h5', custom_objects={'KerasLayer': hub.KerasLayer})\n",
    "\n",
    "# 2. 이미지 전처리\n",
    "def preprocess_image(img_path, target_size=(224, 224)):\n",
    "    img = image.load_img(img_path, target_size=target_size) # 이미지 로드 및 크기 조정\n",
    "    img_array = image.img_to_array(img) # 이미지를 numpy 배열로 변환\n",
    "    img_array_expanded_dims = np.expand_dims(img_array, axis=0) # 차원 확장\n",
    "    return tf.keras.applications.mobilenet.preprocess_input(img_array_expanded_dims) # 모델에 맞게 전처리\n",
    "\n",
    "# 3. 이미지를 모델에 입력하고 예측 수행\n",
    "img_path = 'C:/Users/20161/Downloads/페트병.jpg' \n",
    "preprocessed_image = preprocess_image(img_path)\n",
    "predictions = model.predict(preprocessed_image)\n",
    "\n",
    "# 4. 예측 결과 출력 (여기서는 가장 높은 확률을 가진 클래스를 출력합니다)\n",
    "predicted_class = np.argmax(predictions, axis=1)\n",
    "print(\"Predicted class:\", predicted_class)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-20T04:31:55.942936200Z",
     "start_time": "2024-02-20T04:31:54.278178800Z"
    }
   },
   "id": "84ea7574bf2f332"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "47202aae305cf8b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "peslab",
   "language": "python",
   "display_name": "peslab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
